# -*- coding: utf-8 -*-
"""sentiment_analysis(Blackcoffer).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12gnoQ8oAWIf58228Ra6V0oqHAjsnjcve
"""

import numpy as np
import pandas as pd
from transformers import pipeline,DistilBertTokenizerFast
import requests
from bs4 import BeautifulSoup,NavigableString
import os
import hashlib
from nltk.corpus import cmudict
import string
import nltk
nltk.download()
from nltk.tokenize import word_tokenize
import openpyxl
import re

dataset=pd.read_excel('/content/Input.xlsx')
l=[]
for i in dataset['URL']:
  l.append(i)
l.insert(0,0)

len(l)

def extract(url):
  response =requests.get(url)
  if response.status_code==200:
    soup=BeautifulSoup(response.text,'html.parser')
    elements=soup.find('div',class_='td-post-content tagdiv-type')
    if elements:
      for tag in elements.find(['p', 'strong', 'u','ol']):
        if not isinstance(tag,NavigableString):
          tag.unwrap()
      clean_elements=elements.get_text(strip=True)
      return clean_elements
  else:
    print("didnt worked")
    return "NULL"

def process_input_file(input_file_path, output_folder):
  workbook = openpyxl.load_workbook(input_file_path)
  sheet = workbook.active
  for row in sheet.iter_rows(min_row=2, values_only=True):
    url_id, url = row
    extracted_text = extract(url)
    if extracted_text:
        filename = f"{url_id}.txt"
        filepath = os.path.join(output_folder, filename)
        with open(filepath, 'w', encoding='utf-8') as output_file:
            output_file.write(extracted_text)
        print(f'Text extracted for URL ID {url_id} and saved to {filename}')
    else:
        filename = f"{url_id}.txt"
        filepath = os.path.join(output_folder, filename)
        with open(filepath, 'w', encoding='utf-8') as output_file:
            output_file.write("NULL")
        print(f'Text extracted for URL ID {url_id} and saved to {filename}')
  workbook.close()
input_file_path = '/content/Input.xlsx'  # Replace with the path to your input file
output_folder = '/content/sample_data/output_folder'  # Replace with your desired output folder

process_input_file(input_file_path, output_folder)

#import shutil
#folder_path = '/content/sample_data/output_folder'
#shutil.rmtree(folder_path)

def count_sentences(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    sentences = nltk.sent_tokenize(content)
    return len(sentences)

def personal_pronouns(file_path):
  pronoun_pattern=r'\b(?:I|we|my|ours|us)\b'
  try:
    with open(file_path, 'r', encoding='utf-8') as file:
          paragraph = file.read()
    matches=re.findall(pronoun_pattern,paragraph,flags=re.IGNORECASE)
    matches=[match for match in matches if match.lower()!= 'us']
    return len(matches)
  except FileNotFoundError:
    return 0

def average_word_length(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        words = text.split()
        total_characters = sum(len(word) for word in words)
        total_words = len(words)
        if total_words > 0:
            average_word_length = total_characters / total_words
            return average_word_length
        else:
            return 0
    except FileNotFoundError:
        print(f"File not found: {file_path}")
        return 0

def average_words_per_sentence_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        paragraph = file.read()
    sentences = nltk.sent_tokenize(paragraph)
    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)
    if sentences:
        average_words = total_words / len(sentences)
        return average_words
    else:
        return 0

negative_path='/content/master_dictionary/negative-words.txt'
negative=[]
with open(negative_path,"r",encoding='latin-1')as file:
  file_content=file.read()
  negative=file_content.split()

positive_path='/content/master_dictionary/positive-words.txt'
positive=[]
with open(positive_path,"r",encoding='latin-1')as file:
  file_content=file.read()
  positive=file_content.split()

stopwords = open('StopWords.txt',encoding='latin-1').read()
stopwords=stopwords.split('\n')

def tokenize(j):
  filename=f"blackassign{j:04d}.txt"
  folder_path='/content/sample_data/output_folder'
  file_path=os.path.join(folder_path,filename)
  if os.path.exists(file_path):
    with open(file_path,encoding="utf-8")as file:
      text=file.read().lower()
      text=text.translate(str.maketrans("","",string.punctuation))
      tokenized_text=word_tokenize(text,"english")
      final_words=[word for word in tokenized_text if word not in stopwords]
    return final_words
  else:
    print(f"File not found:{filename}")
    return []

result_dict={}
for j in range(1,102):
  try:
    result_dict[f"text_{j}"]=tokenize(j)
    print(result_dict.get(f'text_{j}',[]))
  except KeyError:
    print(f"Key 'text_{j} not found.")

def complex(word_list):
    cmu_dict = cmudict.dict()
    def is_complex(word):
        try:
            syllables = [len(phoneme) for phoneme in cmu_dict.get(word, [])]
            return sum(syllables) >= 2
        except IndexError:
            return False
    total_complex_words = sum(is_complex(word) for word in word_list)
    return total_complex_words

def count_syllables(word):
    if isinstance(word, str) and word.endswith(('es', 'ed')):
        return 0
    vowels = "aeiouAEIOU"
    count = 0
    is_vowel = False
    for char in word:
        if char in vowels:
            if not is_vowel:
                count += 1
                is_vowel = True
        else:
            is_vowel = False
    return count
def total_syllables_in_value(value):
    total_syllables = 0

    if isinstance(value, list):
        for word in value:
            total_syllables += count_syllables(word)

    return total_syllables

data=[]
count_of_words=[]
avg_syllable=[]
percentage_complex_words=[]
list_total_complex_words=[]
average_word_count_setence=[]
personal_pronouns_count=[]
average_word_len=[]
sentences_count=[]
word_count_before_cleaning=[]
complex_word_count=[]
word_count = 0
for j in range(1, 101):
    target_key = f'text_{j}'
    file_path = f'/content/sample_data/output_folder/blackassign{j:04d}.txt'
    with open(file_path, 'r', encoding='utf-8') as file:
      file_content = file.read()
      words = file_content.split()
      word_count_before = len(words)
      word_count_before_cleaning.append(word_count_before)
    if target_key in result_dict:
        values_for_key = result_dict[target_key]

        # Count of total words after tokenizing
        count_of_values_for_key = len(values_for_key)
        count_of_words.append(count_of_values_for_key)

        # Average syllables per word
        avg_syllables = total_syllables_in_value(values_for_key)
        avg_syllable.append(avg_syllables)

        # Percentage complex words
        word_list = result_dict.get(target_key, [])
        result = complex(word_list)
        complex_word_count.append(result)
        list_total_complex_words.append(result)

        if len(word_list) != 0:
            percentage_complex_words.append((result / len(word_list)) * 100)
        else:
            percentage_complex_words.append(0)

    else:
        print(f"{target_key} not found in the dictionary.")

    try:
        # Average word count in a sentence
        sentences = count_sentences(file_path)
        average_word_count_setence.append(sentences)

        # Personal pronouns count
        pronouns = personal_pronouns(file_path)
        personal_pronouns_count.append(pronouns)

        # Average word length
        word_length = average_word_length(file_path)
        average_word_len.append(word_length)

        # Sentence count
        sentences = count_sentences(file_path)
        sentences_count.append(sentences)

    except FileNotFoundError:
        print(f'file not found: {file_path}')
result_df=pd.DataFrame(data)
result_df.to_excel('Output.xlsx',index=False)



for j in range(1, 101):
    p = 0
    n = 0
    pol = 0
    sub = 0
    asl = 0
    pcw = 0
    fi = 0

    a=result_dict.get(f'text_{j}',[])
    for term in a:
        if term in positive:
            p += 1
        elif term in negative:
            n += 1

            pol = (p - n) / ((p + n) + 0.000001)
            sub = (p + n) / ((len(a)) + 0.000001)
            asl = word_count_before_cleaning[j] / sentences_count[j]
            pcw = list_total_complex_words[j] / word_count_before_cleaning[j]
            fi = 0.4 * (sentences_count[j] / percentage_complex_words[j])
            anwps=average_word_count_setence[j]
            cwc=complex_word_count[j]
            wc=word_count_before_cleaning[j]
            spw=avg_syllable[j]
            pp=personal_pronouns_count[j]
            awl=average_word_len[j]
    data.append({
        'URL': l[j],
        'POSITIVE SCORE': p,
        'NEGATIVE SCORE': n,
        'Polarity Score': pol,
        'Subjectivity Score': sub,
        'Average Sentence Length': asl,
        'Percentage of Complex words': pcw,
        'Fog Index': fi,
        'AVG NUMBER OF WORDS PER SENTENCE':anwps,
        'COMPLEX WORD COUNT':cwc,
        'WORD COUNT':wc,
        'SYLLABLE PER WORD':spw,
        'PERSONAL PRONOUNS':pp,
        'AVG WORD LENGTH':awl
    })

# Create a DataFrame and save it to an Excel file
result_df = pd.DataFrame(data)
result_df.to_excel('Output.xlsx', index=False)